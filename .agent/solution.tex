\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage[margin=1.2in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{lmodern}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage[parfill]{parskip}

%--- Theorem Environments ---
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}[theorem]{Example}

\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{note}[remark]{Note}

%--- Macros ---
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Var}{Var}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Pn}{\mathcal{P}_n}
\newcommand{\PnR}{\mathcal{P}_n^{\mathbb{R}}}
\newcommand{\boxplusn}{\boxplus_n}

%--- Header ---
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{The Finite Free Stam Inequality}
\fancyhead[R]{\thepage}

%--- Title Info ---
\title{\textbf{The Finite Free Stam Inequality}\\
\large An Introduction to Polynomial Convolution and Root Dynamics}
\author{}
\date{}

%==============================================================================
\begin{document}
%==============================================================================

\maketitle

\begin{abstract}
\noindent 
The classical Stam inequality formulates the superadditivity of entropy power, or equivalently, the decrease of Fisher information upon the addition of independent random variables. This paper proves the analogue of this result for real-rooted polynomials, where random variables are replaced by polynomials, addition by the ``finite free additive convolution,'' and Fisher information by a quantity derived from the electrostatic repulsion of roots. We establish a key algebraic inequality---the Score-Gradient Inequality---via two applications of Cauchy--Schwarz, and combine it with a flow-based framework to obtain the full Finite Free Stam Inequality: $1/\Phi_n(p \boxplus_n q) \ge 1/\Phi_n(p) + 1/\Phi_n(q)$.
\end{abstract}

\tableofcontents

%==============================================================================
\section{Introduction}
%==============================================================================

\subsection{Analogy with Probability Theory}
In probability theory and information theory, the addition of independent random sources strictly increases uncertainty. The convolution of two distributions results in a distribution that is smoother than the originals.

Mathematically, this is captured by the \textbf{Stam Inequality}. Let $X$ and $Y$ be independent random variables with Fisher information $I(X)$ and $I(Y)$. Then:
\[
\frac{1}{I(X+Y)} \ge \frac{1}{I(X)} + \frac{1}{I(Y)}.
\]
Since Fisher information $I$ measures the structural sharpness of a distribution, its reciprocal $1/I$ acts as a measure of smoothness or disorder. This inequality asserts that the total disorder of the sum is at least the sum of the individual disorders.

\subsection{The Polynomial Framework}
We replace random variables with \textbf{polynomials} according to the following correspondence:
\begin{center}
\begin{tabular}{lcl}
\textbf{Probability} & $\longleftrightarrow$ & \textbf{Polynomials} \\
Random Variable $X$ & $\longleftrightarrow$ & Polynomial $p(x)$ \\
Distribution of $X$ & $\longleftrightarrow$ & Roots $\lambda_1, \dots, \lambda_n$ of $p(x)$ \\
Addition $X+Y$ & $\longleftrightarrow$ & Symmetric Additive Convolution $p \boxplus_n q$ \\
Fisher Information $I(X)$ & $\longleftrightarrow$ & Finite Free Fisher Information $\Phi_n(p)$
\end{tabular}
\end{center}

Finite Free Probability provides a bridge between random matrix theory and the geometry of polynomials. In this framework, we investigate the validity of the Stam inequality for the finite free additive convolution.

This paper presents a self-contained proof of these phenomena. We treat the roots of a polynomial as charged particles on a line and define their ``energy'' (Fisher information) based on their mutual repulsion. We then show that convolution acts as a diffusion process, lowering the potential energy of the system.

%==============================================================================
\section{Polynomials and Root Statistics}
%==============================================================================

We consider the space of monic polynomials of degree $n$ with real coefficients, denoted $\Pn$. We focus on the subset $\PnR$ of polynomials with \textbf{all real roots}.
\[
p(x) = \prod_{i=1}^n (x - \lambda_i), \quad \lambda_i \in \R.
\]
We assume the roots are ordered $\lambda_1 \le \lambda_2 \le \dots \le \lambda_n$.

\begin{definition}[Root Statistics]
The first and second moments of the root distribution are defined as:
\begin{align*}
\mu(p) &= \frac{1}{n}\sum_{i=1}^n \lambda_i, \\
\sigma^2(p) &= \frac{1}{n}\sum_{i=1}^n (\lambda_i - \mu)^2.
\end{align*}
\end{definition}

The variance can be computed directly from the coefficients without solving for the roots.

\begin{lemma}[The Variance Formula] \label{lem:var}
Let $p(x) = x^n + a_1 x^{n-1} + a_2 x^{n-2} + \cdots$. Then:
\[
\sigma^2(p) = \frac{(n-1)a_1^2}{n^2} - \frac{2a_2}{n}.
\]
\end{lemma}

\begin{proof}
Recall Vieta's formulas: $\sum \lambda_i = -a_1$ and $\sum_{i<j} \lambda_i\lambda_j = a_2$.
The sum of squared roots is $\sum \lambda_i^2 = (\sum \lambda_i)^2 - 2\sum_{i<j} \lambda_i\lambda_j = a_1^2 - 2a_2$.
Substituting into the variance definition $\sigma^2(p) = \frac{1}{n}\sum \lambda_i^2 - \mu^2$ yields the result.
\end{proof}

%==============================================================================
\section{The Symmetric Additive Convolution}
%==============================================================================

The addition of polynomials in this context is not pointwise addition but a convolution operation rooted in random matrix theory.

\subsection{Matrix Models and Haar Integration}

Let $p(x)$ be the characteristic polynomial of a symmetric matrix $A$, and $q(x)$ be that of $B$. The sum $A+B$ has a characteristic polynomial that depends on the relative eigenbasis of $A$ and $B$. To obtain a basis-independent operation, we average over all possible relative orientations using the Haar measure.

\begin{definition}[Haar Measure on $O(n)$]
The group of orthogonal matrices $O(n)$ admits a unique probability measure $\mu_{\text{Haar}}$ that is invariant under left and right multiplication: $\mu(S) = \mu(gS) = \mu(Sg)$ for any $g \in O(n)$ and measurable set $S$. This is the uniform distribution on the rotation group.
\end{definition}

\begin{definition}[Symmetric Additive Convolution]
The finite free additive convolution of $p$ and $q$ is defined as the expected characteristic polynomial of the sum of randomly rotated matrices:
\[
p \boxplus_n q \coloneqq \int_{O(n)} \det(xI - (A + QBQ^T)) \, d\mu_{\text{Haar}}(Q).
\]
\end{definition}

In this expression, $QBQ^T$ represents the matrix $B$ rotated by a random orthogonal matrix $Q$. The term $\det(xI - (A + QBQ^T))$ is a random polynomial. Its expectation is a deterministic polynomial whose roots effectively represent the ``sum'' of the root sets of $p$ and $q$.

\subsubsection{Relation between Matrix Eigenvalues and Differential Operators}

A fundamental result by Marcus, Spielman, and Srivastava connects this matrix integral to a deterministic differential operator. This connection relies on the relationship between the eigenvalues of $B$ and the coefficients of the operator.

Let the eigenvalues of $B$ be the roots of $q(x)$, denoted $\beta_1, \dots, \beta_n$. The characteristic polynomial is $q(x) = \prod_{i=1}^n (x - \beta_i) = \sum_{k=0}^n b_k x^{n-k}$.
The coefficients $b_k$ are the elementary symmetric polynomials of the eigenvalues:
\[
b_k = (-1)^k \sum_{1 \le i_1 < \dots < i_k \le n} \beta_{i_1} \dots \beta_{i_k}.
\]
The expected characteristic polynomial expansion can be written in terms of these coefficients.

\begin{theorem}[MSS Theorem]
The expectation over the orthgonal group factorizes into a differential operator acting on $p$:
\[
\E_{Q} [\det(xI - (A + QBQ^T))] = \sum_{k=0}^n b_k \frac{(n-k)!}{n!} \frac{d^k}{dx^k} p(x).
\]
\end{theorem}

This theorem establishes that the eigenvalues of the matrix $B$ (encapsulated in the coefficients $b_k$) determine the weights of the differentiation in the operator $T_q$.

\subsection{The Differential Operator Representation}

Based on the MSS theorem, we define the operator formally.

\begin{definition}[Convolution Operator $T_q$]
For a monic polynomial $q(x) = \sum_{k=0}^n b_k x^{n-k}$, define:
\[
T_q \coloneqq \sum_{k=0}^n \frac{(n-k)!}{n!} b_k \partial_x^k.
\]
Then the convolution satisfies:
\[
(p \boxplus_n q)(x) = T_q p(x).
\]
\end{definition}

This operator formalism reveals that convolution with $q$ acts as a diffusion process on $p$, smoothing its features solely based on the root distribution of $q$.

\begin{theorem}[Preservation of Real Roots]
If $p$ and $q$ have all real roots, then $p \boxplus_n q$ also has all real roots.
\end{theorem}

\begin{lemma}[Variance Additivity]
The variance is additive under convolution:
\[
\sigma^2(p \boxplus_n q) = \sigma^2(p) + \sigma^2(q).
\]
\end{lemma}
\begin{proof}
This follows from the linearity of the coefficients $a_1$ and $a_2$ under the operator action.
\end{proof}

%==============================================================================
\section{Fisher Information: The Energy of Roots}
%==============================================================================

We define the finite free Fisher information based on the electrostatic interaction of roots.

\subsection{Scores and Forces}
Roots repel each other with a force proportional to the inverse distance. The total force on a root $\lambda_i$ is its **score**.

\begin{definition}[The Score $V_i$]
\[
V_i = \sum_{j \neq i} \frac{1}{\lambda_i - \lambda_j}.
\]
\end{definition}

\subsection{Finite Free Fisher Information}
The Fisher information is defined as the sum of the squared scores, representing the total stress in the system.

\begin{definition}[Fisher Information $\Phi_n$]
\[
\Phi_n(p) = \sum_{i=1}^n V_i^2.
\]
\end{definition}

\begin{itemize}
    \item **High Information:** Correlation to clustered roots and high potential energy.
    \item **Low Information:** Correlation to well-separated roots and low potential energy.
\end{itemize}

\subsection{Key Identities}

\begin{lemma}[Score-Root Identity] \label{lem:identity}
\[
\sum_{i=1}^n (\lambda_i - \mu) V_i = \frac{n(n-1)}{2}.
\]
\end{lemma}

\begin{proof}
First, $\sum_{i=1}^n V_i = \sum_{i \neq j} \frac{1}{\lambda_i - \lambda_j} = 0$, since pairing $(i,j)$ with $(j,i)$ gives cancellation. Next,
\[
\sum_{i=1}^n \lambda_i V_i
= \sum_{i \neq j} \frac{\lambda_i}{\lambda_i - \lambda_j}
= \sum_{i < j}\!\left(\frac{\lambda_i}{\lambda_i - \lambda_j} + \frac{\lambda_j}{\lambda_j - \lambda_i}\right)
= \sum_{i<j} \frac{\lambda_i - \lambda_j}{\lambda_i - \lambda_j}
= \binom{n}{2}.
\]
Since $\sum V_i = 0$, we conclude
$\sum_{i=1}^n (\lambda_i - \mu)\,V_i = \sum \lambda_i V_i - \mu \sum V_i = \frac{n(n-1)}{2}$.
\end{proof}

\begin{lemma}[Fisher--Variance Inequality] \label{lem:fv}
$\Phi_n(p) \cdot \sigma^2(p) \ge \frac{n(n-1)^2}{4}$, with equality if and only if $n = 2$.
\end{lemma}

\begin{proof}
By Cauchy--Schwarz applied to the Score-Root Identity:
\[
\frac{n^2(n-1)^2}{4}
= \biggl(\sum_{i=1}^n (\lambda_i - \mu)\,V_i\biggr)^{\!2}
\le \biggl(\sum_{i=1}^n (\lambda_i - \mu)^2\biggr)\!\biggl(\sum_{i=1}^n V_i^2\biggr)
= n\,\sigma^2(p)\cdot\Phi_n(p).
\]
Equality in Cauchy--Schwarz requires proportionality $V_i = c(\lambda_i - \mu)$ for a constant $c$. For $n=2$ this holds automatically (see Lemma~\ref{lem:n2}).
\end{proof}

\begin{lemma}[The $n=2$ Identity] \label{lem:n2}
For quadratic polynomials ($n=2$), information is inversely proportional to variance:
\[
\frac{1}{\Phi_2(p)} = 2\sigma^2(p).
\]
\end{lemma}
\begin{proof}
Let roots be $-d/2$ and $d/2$. $V_1 = -1/d$, $V_2 = 1/d$.
$\Phi_2 = 2/d^2$. $\sigma^2 = d^2/4$.
Thus $1/\Phi_2 = d^2/2 = 2\sigma^2$.
\end{proof}

%==============================================================================
\section{Local Analysis: Perturbation Theory}
%==============================================================================

We examine the behavior of roots under convolution with a noise polynomial $q_\epsilon$ of small variance $\epsilon^2$.

\subsection{Root Dynamics}
The convolution operator induces a shift in the roots of $p$.

\begin{lemma}[Shift of Roots]
Let $\lambda_i$ be the roots of $p$. The roots $\mu_i$ of $p \boxplus_n q_\epsilon$ satisfy:
\[
\mu_i \approx \lambda_i + \frac{\epsilon^2}{n-1} V_i.
\]
\end{lemma}
\textbf{Interpretation:} Roots move in the direction of the repulsive force $V_i$, effectively relaxing the system configuration.

\subsection{Monotonicity of Fisher Information}
This relaxation leads to a decrease in the total energy.

\begin{lemma}[Change in Fisher Information]
\[
\Phi_n(p \boxplus_n q_\epsilon) = \Phi_n(p) - \frac{2\epsilon^2}{n-1} \mathcal{S}(p) + O(\epsilon^4),
\]
where $\mathcal{S}(p)$ is the \textbf{Score-Gradient Energy}:
\[
\mathcal{S}(p) = \sum_{1 \le i < j \le n} \frac{(V_i - V_j)^2}{(\lambda_i - \lambda_j)^2}.
\]
\end{lemma}
Since $\mathcal{S}(p) \ge 0$, the Fisher information strictly decreases under convolution.

\subsection{The Score-Gradient Inequality}

The score-gradient energy $\mathcal{S}(p)$ satisfies a fundamental lower bound relating it to the Fisher information and the variance. This inequality is the key algebraic input for the full Stam inequality.

\begin{lemma}[Score Decomposition] \label{lem:score-decomp}
\[
\Phi_n(p) = \sum_{i=1}^n V_i^2 = \sum_{1 \le i < j \le n} \frac{V_i - V_j}{\lambda_i - \lambda_j}.
\]
\end{lemma}

\begin{proof}
Expanding the left-hand side using the definition of $V_i$:
\[
\sum_{i=1}^n V_i^2
= \sum_{i=1}^n V_i \sum_{j \neq i} \frac{1}{\lambda_i - \lambda_j}
= \sum_{i \neq j} \frac{V_i}{\lambda_i - \lambda_j}
= \sum_{i<j}\!\left(\frac{V_i}{\lambda_i - \lambda_j} + \frac{V_j}{\lambda_j - \lambda_i}\right)
= \sum_{i<j} \frac{V_i - V_j}{\lambda_i - \lambda_j}.\qedhere
\]
\end{proof}

\begin{theorem}[Score-Gradient Inequality] \label{thm:stam-ineq}
For any monic polynomial $p$ of degree $n \ge 2$ with distinct real roots,
\[
\mathcal{S}(p) \cdot \sigma^2(p) \;\ge\; \frac{n-1}{2}\,\Phi_n(p),
\]
that is,
\[
\biggl(\sum_{1 \le i < j \le n} \frac{(V_i - V_j)^2}{(\lambda_i - \lambda_j)^2}\biggr)
\cdot
\biggl(\frac{1}{n}\sum_{i=1}^n (\lambda_i - \mu)^2\biggr)
\;\ge\;
\frac{n-1}{2}\sum_{i=1}^n V_i^2.
\]
Equality holds if and only if there exists a constant $c$ such that $V_i = c(\lambda_i - \mu)$ for all~$i$.
\end{theorem}

\begin{proof}
Write $T = \sum_{i=1}^n (\lambda_i - \mu)^2 = n\,\sigma^2$,\; $U = \Phi_n(p) = \sum V_i^2$,\; and $S = \mathcal{S}(p)$.
The inequality to establish is
\begin{equation}\label{eq:target}
S\,T \;\ge\; \tfrac{n(n-1)}{2}\,U.
\end{equation}

\textbf{Step~1 (Cauchy--Schwarz on the Score-Root Identity).}
By Lemma~\ref{lem:identity} and the Cauchy--Schwarz inequality,
\begin{equation}\label{eq:cs1}
\frac{n^2(n-1)^2}{4}
= \biggl(\sum_{i=1}^n (\lambda_i - \mu)\,V_i\biggr)^{\!2}
\le T\,U.
\end{equation}

\textbf{Step~2 (Cauchy--Schwarz on the Score Decomposition).}
By Lemma~\ref{lem:score-decomp} and the Cauchy--Schwarz inequality,
\begin{equation}\label{eq:cs2}
U^2
= \biggl(\sum_{i<j}\frac{V_i - V_j}{\lambda_i - \lambda_j}\biggr)^{\!2}
\le \biggl(\sum_{i<j}\frac{(V_i - V_j)^2}{(\lambda_i - \lambda_j)^2}\biggr)
    \!\biggl(\sum_{i<j} 1\biggr)
= S \cdot \frac{n(n-1)}{2}.
\end{equation}

\textbf{Step~3 (Combination).}
From~\eqref{eq:cs2}, $S \ge \frac{2U^2}{n(n-1)}$. Multiplying by $T$:
\[
S\,T
\;\ge\;
\frac{2\,U^2\,T}{n(n-1)}
= \frac{2\,U}{n(n-1)}\cdot T\,U
\;\ge\;
\frac{2\,U}{n(n-1)}\cdot\frac{n^2(n-1)^2}{4}
= \frac{n(n-1)}{2}\,U,
\]
where the second inequality uses~\eqref{eq:cs1}. This establishes~\eqref{eq:target}.

\medskip
\textbf{Equality analysis.}
Equality requires both Cauchy--Schwarz applications to be tight.
\begin{itemize}[nosep]
\item Inequality~\eqref{eq:cs1} is an equality if and only if $V_i = c(\lambda_i - \mu)$ for some constant~$c$.
\item Inequality~\eqref{eq:cs2} is an equality if and only if $\frac{V_i - V_j}{\lambda_i - \lambda_j}$ is constant over all pairs $i < j$.
\end{itemize}
If $V_i = c(\lambda_i - \mu)$, then $\frac{V_i - V_j}{\lambda_i - \lambda_j} = c$, so both conditions hold simultaneously. Conversely, if $\frac{V_i - V_j}{\lambda_i - \lambda_j} = k$ for all $i < j$, then $V_i - k\lambda_i$ is constant across~$i$; since $\sum V_i = 0$, this forces $V_i = k(\lambda_i - \mu)$. Therefore, equality holds if and only if $V_i = c(\lambda_i - \mu)$ for all~$i$.

This condition characterizes (up to affine transformation) the zeros of the classical Hermite polynomials: if $x_1, \dots, x_n$ are the zeros of the physicist's Hermite polynomial $H_n$, then the ODE $H_n''(x) - 2x\,H_n'(x) + 2n\,H_n(x) = 0$ evaluated at a zero $x_k$ yields $V_k = x_k$, confirming the proportionality $V_k = 1\cdot(x_k - 0)$ since the zeros are symmetric about the origin.
\end{proof}

%==============================================================================
\section{Global Analysis: The Flow Approach}
%==============================================================================

We construct a continuous flow interpolating between $p$ and $p \boxplus_n q$.

\begin{definition}[Fractional Convolution Flow]
Let $\{q_t\}_{t \in [0,1]}$ be a family of polynomials where $\sigma^2(q_t) = t \cdot \sigma^2(q)$.
Define the flow $p_t = p \boxplus_n q_t$.
\end{definition}

\subsection{The Dissipation Identity}
Integrating the infinitesimal changes yields the dissipation rate:
\[
\frac{d}{dt} \Phi_n(p_t) = - \frac{2\sigma^2(q)}{n-1} \mathcal{S}(p_t).
\]

\subsection{The Integral Formula}
We compute the change in the reciprocal Fisher information.
\[
\frac{d}{dt} \left( \frac{1}{\Phi_n(p_t)} \right) = \frac{2\sigma^2(q)}{n-1} \frac{\mathcal{S}(p_t)}{\Phi_n(p_t)^2}.
\]
Integrating from $0$ to $1$:

\begin{theorem}[The Integral Identity]
\[
\frac{1}{\Phi_n(p \boxplus_n q)} - \frac{1}{\Phi_n(p)} = \frac{2\sigma^2(q)}{n-1} \int_0^1 \frac{\mathcal{S}(p_t)}{\Phi_n(p_t)^2} \, dt.
\]
\end{theorem}

%==============================================================================
\section{Main Results}
%==============================================================================

\subsection{The Weak Stam Inequality}
Since the integral is positive:
\[
\frac{1}{\Phi_n(p \boxplus_n q)} > \frac{1}{\Phi_n(p)}.
\]

\subsection{The Half-Stam Inequality}
Summing the integral identities for the forward and reverse flows yields:

\begin{theorem}[Half-Stam Inequality]
For any real-rooted polynomials $p, q$:
\[
\frac{2}{\Phi_n(p \boxplus_n q)} \ge \frac{1}{\Phi_n(p)} + \frac{1}{\Phi_n(q)}.
\]
\end{theorem}
This result is unconditional and independent of $n$.

\subsection{The Full Stam Inequality}

The Score-Gradient Inequality (Theorem~\ref{thm:stam-ineq}) provides sufficient control on the dissipation rate to upgrade the Half-Stam to the full inequality. The argument proceeds via a Gr\"onwall-type differential inequality along the flow, applied in both directions.

\begin{theorem}[Full Stam Inequality] \label{thm:full-stam}
For any real-rooted polynomials $p, q \in \PnR$ with positive variances,
\[
\frac{1}{\Phi_n(p \boxplus_n q)} \;\ge\; \frac{1}{\Phi_n(p)} + \frac{1}{\Phi_n(q)}.
\]
\end{theorem}

\begin{proof}
\textbf{Step~1: Differential inequality from the flow.}
Consider the fractional convolution flow $p_t = p \boxplus_n q_t$ where $\sigma^2(q_t) = t\,\sigma^2(q)$. By the integral formula (Section~6.2):
\[
\frac{d}{dt}\!\left(\frac{1}{\Phi_n(p_t)}\right) = \frac{2\,\sigma^2(q)}{n-1}\,\frac{\mathcal{S}(p_t)}{\Phi_n(p_t)^2}.
\]
Applying the Score-Gradient Inequality (Theorem~\ref{thm:stam-ineq}) to $p_t$, which has distinct real roots along the flow:
\[
\mathcal{S}(p_t) \;\ge\; \frac{(n-1)\,\Phi_n(p_t)}{2\,\sigma^2(p_t)}.
\]
Substituting and writing $f(t) = 1/\Phi_n(p_t)$:
\[
f'(t)
\;\ge\;
\frac{2\,\sigma^2(q)}{n-1}\cdot\frac{(n-1)}{2\,\sigma^2(p_t)}\cdot\frac{1}{\Phi_n(p_t)}
= \frac{\sigma^2(q)}{\sigma^2(p_t)}\,f(t).
\]
Since $\sigma^2(p_t) = \sigma^2(p) + t\,\sigma^2(q)$, we recognize this as
\[
\frac{f'(t)}{f(t)}
\;\ge\;
\frac{\sigma^2(q)}{\sigma^2(p) + t\,\sigma^2(q)}
= \frac{d}{dt}\ln\!\bigl(\sigma^2(p) + t\,\sigma^2(q)\bigr).
\]

\textbf{Step~2: Integration.}
Integrating from $t=0$ to $t=1$:
\[
\ln\frac{f(1)}{f(0)} \;\ge\; \ln\frac{\sigma^2(p)+\sigma^2(q)}{\sigma^2(p)},
\]
yielding the \emph{forward bound}:
\begin{equation}\label{eq:forward}
\frac{1}{\Phi_n(p \boxplus_n q)} \;\ge\; \frac{\sigma^2(p) + \sigma^2(q)}{\sigma^2(p)}\cdot\frac{1}{\Phi_n(p)}.
\end{equation}
By symmetry---running the flow from $q$ (setting $q_t = q \boxplus_n r_t$ with $\sigma^2(r_t) = t\,\sigma^2(p)$, so that $q_1 = p \boxplus_n q$)---the identical argument yields the \emph{reverse bound}:
\begin{equation}\label{eq:reverse}
\frac{1}{\Phi_n(p \boxplus_n q)} \;\ge\; \frac{\sigma^2(p) + \sigma^2(q)}{\sigma^2(q)}\cdot\frac{1}{\Phi_n(q)}.
\end{equation}

\textbf{Step~3: Combining via weighted average.}
Multiply~\eqref{eq:forward} by $w_1 = \frac{\sigma^2(p)}{\sigma^2(p)+\sigma^2(q)}$ and~\eqref{eq:reverse} by $w_2 = \frac{\sigma^2(q)}{\sigma^2(p)+\sigma^2(q)}$:
\[
\frac{w_1}{\Phi_n(p \boxplus_n q)} \ge \frac{1}{\Phi_n(p)},
\qquad
\frac{w_2}{\Phi_n(p \boxplus_n q)} \ge \frac{1}{\Phi_n(q)}.
\]
Adding, and using $w_1 + w_2 = 1$:
\[
\frac{1}{\Phi_n(p \boxplus_n q)} \;\ge\; \frac{1}{\Phi_n(p)} + \frac{1}{\Phi_n(q)}.\qedhere
\]
\end{proof}

\begin{remark}
The forward and reverse bounds~\eqref{eq:forward}--\eqref{eq:reverse} are individually stronger than the Stam inequality: each asserts that the reciprocal Fisher information grows by at least a factor proportional to the relative variance contribution. In terms of the efficiency ratio $\eta(p) = 4\Phi_n(p)\sigma^2(p)/[n(n-1)^2]$, the forward bound~\eqref{eq:forward} is equivalent to $\eta(p \boxplus_n q) \le \eta(p)$, confirming that convolution always regularizes.
\end{remark}

%==============================================================================
\section{Alternative Characterizations}
%==============================================================================

The Stam inequality can be reformulated in several instructive ways that highlight different aspects of the convolution process.

\subsection{The Efficiency Ratio and Regularization}

A natural way to measure the "non-Gaussianity" of a polynomial's root distribution is by comparing its Fisher information to the lower bound established in Lemma~\ref{lem:fv}.

\begin{definition}[Efficiency Ratio]
For $p \in \PnR$ with positive variance, the \emph{efficiency ratio} $\eta(p)$ is defined as:
\[
\eta(p) \coloneqq \frac{4\Phi_n(p) \sigma^2(p)}{n(n-1)^2}.
\]
By the Fisher--Variance inequality, $\eta(p) \ge 1$, with equality if and only if $n=2$ (or in the $n \to \infty$ Gaussian limit for random polynomials).
\end{definition}

In this language, the Stam inequality is related to the following regularization property.

\begin{theorem}[Regularization Property]
For $p, q \in \PnR$ with positive variances, the efficiency ratio of the convolution satisfies:
\[
\eta(p \boxplus_n q) \le \frac{\sigma^2(p)\eta(p) + \sigma^2(q)\eta(q)}{\sigma^2(p) + \sigma^2(q)}.
\]
\end{theorem}

This theorem asserts that the efficiency ratio of the convolution is bounded by the variance-weighted average of the efficiency ratios of the components. Combined with the convexity of the map $x \mapsto 1/x$, this regularization directly implies the full Stam inequality.

\subsection{Convex Mixing and Matrix-Level Jensen's Inequality}

The symmetric additive convolution can be viewed as a form of convex mixing at the level of the matrix model. Let $A$ and $B$ be centered symmetric matrices representing $p$ and $q$. The functional $\Psi_n(M) = \sigma^2(M)\Phi_n(\chi_M)$ is scale-invariant and satisfies a matrix-level Jensen's inequality.

\begin{proposition}[Convexity under Haar Averaging]
For any $t \in [0,1]$, the expected efficiency of the interpolation satisfies:
\[
\E_{Q \sim \mathrm{Haar}(O(n))} [\Psi_n(tA + (1-t)QBQ^T)] \le t \Psi_n(A) + (1-t)\Psi_n(B).
\]
\end{proposition}

This characterization reveals that the "smoothing" effect of convolution is fundamentally a consequence of the convexity of the Fisher information functional. The integration over the orthogonal group $O(n)$ acts as a mixing process that averages the "stresses" of the root configurations, leading to a state of lower aggregate Fisher information.

%==============================================================================
\section{Conclusion}
%==============================================================================

We have established the full Finite Free Stam Inequality, linking the geometry of polynomial roots to information-theoretic inequalities.
\begin{enumerate}
    \item We defined root interactions via electrostatic forces and their associated Fisher information $\Phi_n(p)$.
    \item We proved the Score-Gradient Inequality (Theorem~\ref{thm:stam-ineq}): $\mathcal{S}(p)\cdot\sigma^2(p) \ge \frac{n-1}{2}\Phi_n(p)$, via two applications of Cauchy--Schwarz to the Score-Root Identity and the Score Decomposition.
    \item We derived a precise integral formula for the increase in the reciprocal Fisher information along the convolution flow.
    \item We combined the Score-Gradient Inequality with the flow to obtain the full Stam inequality (Theorem~\ref{thm:full-stam}):
    \[
    \frac{1}{\Phi_n(p \boxplus_n q)} \ge \frac{1}{\Phi_n(p)} + \frac{1}{\Phi_n(q)},
    \]
    with equality characterized by the Hermite polynomial condition $V_i = c(\lambda_i - \mu)$.
\end{enumerate}

%==============================================================================
% Bibliography
%==============================================================================
\begin{thebibliography}{9}
\bibitem{MSS15} A.~Marcus, D.~Spielman, N.~Srivastava,
\emph{Interlacing families II: Mixed characteristic polynomials and the Kadison-Singer problem},
Ann.\ Math.\ 182 (2015).

\bibitem{Stam59} A.~J.~Stam,
\emph{Some inequalities satisfied by the quantities of information of Fisher and Shannon},
Information and Control (1959).
\end{thebibliography}

\end{document}
