\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage[margin=1.2in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{lmodern}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage[parfill]{parskip}

%--- Theorem Environments ---
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}[theorem]{Example}

\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{note}[remark]{Note}

%--- Macros ---
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Var}{Var}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Pn}{\mathcal{P}_n}
\newcommand{\PnR}{\mathcal{P}_n^{\mathbb{R}}}
\newcommand{\boxplusn}{\boxplus_n}

%--- Header ---
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{The Finite Free Stam Inequality}
\fancyhead[R]{\thepage}

%--- Title Info ---
\title{\textbf{The Finite Free Stam Inequality}\\
\large An Introduction to Polynomial Convolution and Root Dynamics}
\author{}
\date{}

%==============================================================================
\begin{document}
%==============================================================================

\maketitle

\begin{abstract}
\noindent 
The classical Stam inequality formulates the superadditivity of entropy power, or equivalently, the decrease of Fisher information upon the addition of independent random variables. This paper explores the analogue of this result for real-rooted polynomials, where random variables are replaced by polynomials, addition by the ``finite free additive convolution,'' and Fisher information by a quantity derived from the electrostatic repulsion of roots. We develop a flow-based framework to analyze the evolution of roots under convolution, demonstrating that they evolve to minimize their interaction energy. We establish a ``Half-Stam'' inequality unconditionally and reduce the full conjecture to a specific cubic control inequality.
\end{abstract}

\tableofcontents

%==============================================================================
\section{Introduction}
%==============================================================================

\subsection{Analogy with Probability Theory}
In probability theory and information theory, the addition of independent random sources strictly increases uncertainty. The convolution of two distributions results in a distribution that is smoother than the originals.

Mathematically, this is captured by the \textbf{Stam Inequality}. Let $X$ and $Y$ be independent random variables with Fisher information $I(X)$ and $I(Y)$. Then:
\[
\frac{1}{I(X+Y)} \ge \frac{1}{I(X)} + \frac{1}{I(Y)}.
\]
Since Fisher information $I$ measures the structural sharpness of a distribution, its reciprocal $1/I$ acts as a measure of smoothness or disorder. This inequality asserts that the total disorder of the sum is at least the sum of the individual disorders.

\subsection{The Polynomial Framework}
We replace random variables with \textbf{polynomials} according to the following correspondence:
\begin{center}
\begin{tabular}{lcl}
\textbf{Probability} & $\longleftrightarrow$ & \textbf{Polynomials} \\
Random Variable $X$ & $\longleftrightarrow$ & Polynomial $p(x)$ \\
Distribution of $X$ & $\longleftrightarrow$ & Roots $\lambda_1, \dots, \lambda_n$ of $p(x)$ \\
Addition $X+Y$ & $\longleftrightarrow$ & Symmetric Additive Convolution $p \boxplus_n q$ \\
Fisher Information $I(X)$ & $\longleftrightarrow$ & Finite Free Fisher Information $\Phi_n(p)$
\end{tabular}
\end{center}

Finite Free Probability provides a bridge between random matrix theory and the geometry of polynomials. In this framework, we investigate the validity of the Stam inequality for the finite free additive convolution.

This paper presents a self-contained proof of these phenomena. We treat the roots of a polynomial as charged particles on a line and define their ``energy'' (Fisher information) based on their mutual repulsion. We then show that convolution acts as a diffusion process, lowering the potential energy of the system.

%==============================================================================
\section{Polynomials and Root Statistics}
%==============================================================================

We consider the space of monic polynomials of degree $n$ with real coefficients, denoted $\Pn$. We focus on the subset $\PnR$ of polynomials with \textbf{all real roots}.
\[
p(x) = \prod_{i=1}^n (x - \lambda_i), \quad \lambda_i \in \R.
\]
We assume the roots are ordered $\lambda_1 \le \lambda_2 \le \dots \le \lambda_n$.

\begin{definition}[Root Statistics]
The first and second moments of the root distribution are defined as:
\begin{align*}
\mu(p) &= \frac{1}{n}\sum_{i=1}^n \lambda_i, \\
\sigma^2(p) &= \frac{1}{n}\sum_{i=1}^n (\lambda_i - \mu)^2.
\end{align*}
\end{definition}

The variance can be computed directly from the coefficients without solving for the roots.

\begin{lemma}[The Variance Formula] \label{lem:var}
Let $p(x) = x^n + a_1 x^{n-1} + a_2 x^{n-2} + \cdots$. Then:
\[
\sigma^2(p) = \frac{(n-1)a_1^2}{n^2} - \frac{2a_2}{n}.
\]
\end{lemma}

\begin{proof}
Recall Vieta's formulas: $\sum \lambda_i = -a_1$ and $\sum_{i<j} \lambda_i\lambda_j = a_2$.
The sum of squared roots is $\sum \lambda_i^2 = (\sum \lambda_i)^2 - 2\sum_{i<j} \lambda_i\lambda_j = a_1^2 - 2a_2$.
Substituting into the variance definition $\sigma^2(p) = \frac{1}{n}\sum \lambda_i^2 - \mu^2$ yields the result.
\end{proof}

%==============================================================================
\section{The Symmetric Additive Convolution}
%==============================================================================

The addition of polynomials in this context is not pointwise addition but a convolution operation rooted in random matrix theory.

\subsection{Matrix Models and Haar Integration}

Let $p(x)$ be the characteristic polynomial of a symmetric matrix $A$, and $q(x)$ be that of $B$. The sum $A+B$ has a characteristic polynomial that depends on the relative eigenbasis of $A$ and $B$. To obtain a basis-independent operation, we average over all possible relative orientations using the Haar measure.

\begin{definition}[Haar Measure on $O(n)$]
The group of orthogonal matrices $O(n)$ admits a unique probability measure $\mu_{\text{Haar}}$ that is invariant under left and right multiplication: $\mu(S) = \mu(gS) = \mu(Sg)$ for any $g \in O(n)$ and measurable set $S$. This is the uniform distribution on the rotation group.
\end{definition}

\begin{definition}[Symmetric Additive Convolution]
The finite free additive convolution of $p$ and $q$ is defined as the expected characteristic polynomial of the sum of randomly rotated matrices:
\[
p \boxplus_n q \coloneqq \int_{O(n)} \det(xI - (A + QBQ^T)) \, d\mu_{\text{Haar}}(Q).
\]
\end{definition}

In this expression, $QBQ^T$ represents the matrix $B$ rotated by a random orthogonal matrix $Q$. The term $\det(xI - (A + QBQ^T))$ is a random polynomial. Its expectation is a deterministic polynomial whose roots effectively represent the ``sum'' of the root sets of $p$ and $q$.

\subsubsection{Relation between Matrix Eigenvalues and Differential Operators}

A fundamental result by Marcus, Spielman, and Srivastava connects this matrix integral to a deterministic differential operator. This connection relies on the relationship between the eigenvalues of $B$ and the coefficients of the operator.

Let the eigenvalues of $B$ be the roots of $q(x)$, denoted $\beta_1, \dots, \beta_n$. The characteristic polynomial is $q(x) = \prod_{i=1}^n (x - \beta_i) = \sum_{k=0}^n b_k x^{n-k}$.
The coefficients $b_k$ are the elementary symmetric polynomials of the eigenvalues:
\[
b_k = (-1)^k \sum_{1 \le i_1 < \dots < i_k \le n} \beta_{i_1} \dots \beta_{i_k}.
\]
The expected characteristic polynomial expansion can be written in terms of these coefficients.

\begin{theorem}[MSS Theorem]
The expectation over the orthgonal group factorizes into a differential operator acting on $p$:
\[
\E_{Q} [\det(xI - (A + QBQ^T))] = \sum_{k=0}^n b_k \frac{(n-k)!}{n!} \frac{d^k}{dx^k} p(x).
\]
\end{theorem}

This theorem establishes that the eigenvalues of the matrix $B$ (encapsulated in the coefficients $b_k$) determine the weights of the differentiation in the operator $T_q$.

\subsection{The Differential Operator Representation}

Based on the MSS theorem, we define the operator formally.

\begin{definition}[Convolution Operator $T_q$]
For a monic polynomial $q(x) = \sum_{k=0}^n b_k x^{n-k}$, define:
\[
T_q \coloneqq \sum_{k=0}^n \frac{(n-k)!}{n!} b_k \partial_x^k.
\]
Then the convolution satisfies:
\[
(p \boxplus_n q)(x) = T_q p(x).
\]
\end{definition}

This operator formalism reveals that convolution with $q$ acts as a diffusion process on $p$, smoothing its features solely based on the root distribution of $q$.

\begin{theorem}[Preservation of Real Roots]
If $p$ and $q$ have all real roots, then $p \boxplus_n q$ also has all real roots.
\end{theorem}

\begin{lemma}[Variance Additivity]
The variance is additive under convolution:
\[
\sigma^2(p \boxplus_n q) = \sigma^2(p) + \sigma^2(q).
\]
\end{lemma}
\begin{proof}
This follows from the linearity of the coefficients $a_1$ and $a_2$ under the operator action.
\end{proof}

%==============================================================================
\section{Fisher Information: The Energy of Roots}
%==============================================================================

We define the finite free Fisher information based on the electrostatic interaction of roots.

\subsection{Scores and Forces}
Roots repel each other with a force proportional to the inverse distance. The total force on a root $\lambda_i$ is its **score**.

\begin{definition}[The Score $V_i$]
\[
V_i = \sum_{j \neq i} \frac{1}{\lambda_i - \lambda_j}.
\]
\end{definition}

\subsection{Finite Free Fisher Information}
The Fisher information is defined as the sum of the squared scores, representing the total stress in the system.

\begin{definition}[Fisher Information $\Phi_n$]
\[
\Phi_n(p) = \sum_{i=1}^n V_i^2.
\]
\end{definition}

\begin{itemize}
    \item **High Information:** Correlation to clustered roots and high potential energy.
    \item **Low Information:** Correlation to well-separated roots and low potential energy.
\end{itemize}

\subsection{Key Identities}

\begin{lemma}[Score-Root Identity] \label{lem:identity}
\[
\sum_{i=1}^n (\lambda_i - \mu) V_i = \frac{n(n-1)}{2}.
\]
\end{lemma}

\begin{lemma}[Fisher--Variance Inequality] \label{lem:fv}
$\Phi_n(p) \cdot \sigma^2(p) \ge \frac{n(n-1)^2}{4}$, with equality if and only if $n = 2$.
\end{lemma}

\begin{lemma}[The $n=2$ Identity] \label{lem:n2}
For quadratic polynomials ($n=2$), information is inversely proportional to variance:
\[
\frac{1}{\Phi_2(p)} = 2\sigma^2(p).
\]
\end{lemma}
\begin{proof}
Let roots be $-d/2$ and $d/2$. $V_1 = -1/d$, $V_2 = 1/d$.
$\Phi_2 = 2/d^2$. $\sigma^2 = d^2/4$.
Thus $1/\Phi_2 = d^2/2 = 2\sigma^2$.
\end{proof}

%==============================================================================
\section{Local Analysis: Perturbation Theory}
%==============================================================================

We examine the behavior of roots under convolution with a noise polynomial $q_\epsilon$ of small variance $\epsilon^2$.

\subsection{Root Dynamics}
The convolution operator induces a shift in the roots of $p$.

\begin{lemma}[Shift of Roots]
Let $\lambda_i$ be the roots of $p$. The roots $\mu_i$ of $p \boxplus_n q_\epsilon$ satisfy:
\[
\mu_i \approx \lambda_i + \frac{\epsilon^2}{n-1} V_i.
\]
\end{lemma}
\textbf{Interpretation:} Roots move in the direction of the repulsive force $V_i$, effectively relaxing the system configuration.

\subsection{Monotonicity of Fisher Information}
This relaxation leads to a decrease in the total energy.

\begin{lemma}[Change in Fisher Information]
\[
\Phi_n(p \boxplus_n q_\epsilon) = \Phi_n(p) - \frac{2\epsilon^2}{n-1} \mathcal{S}(p) + O(\epsilon^4),
\]
where $\mathcal{S}(p)$ is the \textbf{Score-Gradient Energy}:
\[
\mathcal{S}(p) = \sum_{1 \le i < j \le n} \frac{(V_i - V_j)^2}{(\lambda_i - \lambda_j)^2}.
\]
\end{lemma}
Since $\mathcal{S}(p) \ge 0$, the Fisher information strictly decreases under convolution.

%==============================================================================
\section{Global Analysis: The Flow Approach}
%==============================================================================

We construct a continuous flow interpolating between $p$ and $p \boxplus_n q$.

\begin{definition}[Fractional Convolution Flow]
Let $\{q_t\}_{t \in [0,1]}$ be a family of polynomials where $\sigma^2(q_t) = t \cdot \sigma^2(q)$.
Define the flow $p_t = p \boxplus_n q_t$.
\end{definition}

\subsection{The Dissipation Identity}
Integrating the infinitesimal changes yields the dissipation rate:
\[
\frac{d}{dt} \Phi_n(p_t) = - \frac{2\sigma^2(q)}{n-1} \mathcal{S}(p_t).
\]

\subsection{The Integral Formula}
We compute the change in the reciprocal Fisher information.
\[
\frac{d}{dt} \left( \frac{1}{\Phi_n(p_t)} \right) = \frac{2\sigma^2(q)}{n-1} \frac{\mathcal{S}(p_t)}{\Phi_n(p_t)^2}.
\]
Integrating from $0$ to $1$:

\begin{theorem}[The Integral Identity]
\[
\frac{1}{\Phi_n(p \boxplus_n q)} - \frac{1}{\Phi_n(p)} = \frac{2\sigma^2(q)}{n-1} \int_0^1 \frac{\mathcal{S}(p_t)}{\Phi_n(p_t)^2} \, dt.
\]
\end{theorem}

%==============================================================================
\section{Main Results}
%==============================================================================

\subsection{The Weak Stam Inequality}
Since the integral is positive:
\[
\frac{1}{\Phi_n(p \boxplus_n q)} > \frac{1}{\Phi_n(p)}.
\]

\subsection{The Half-Stam Inequality}
Summing the integral identities for the forward and reverse flows yields:

\begin{theorem}[Half-Stam Inequality]
For any real-rooted polynomials $p, q$:
\[
\frac{2}{\Phi_n(p \boxplus_n q)} \ge \frac{1}{\Phi_n(p)} + \frac{1}{\Phi_n(q)}.
\]
\end{theorem}
This result is unconditional and independent of $n$.

\subsection{The Full Stam Conjecture and Cubic Control}
The optimal bound holds if the function $f(t) = 1/\Phi_n(p_t)$ is concave. This leads to the Cubic Control condition.

\begin{conjecture}[Cubic Control Inequality]
The full Stam inequality holds if:
\[
\sum_{i<j} \frac{(V_i - V_j)^3}{(\lambda_i - \lambda_j)^3} \ge - \frac{\mathcal{S}(p)^2}{\Phi_n(p)}.
\]
\end{conjecture}

\begin{theorem}[Conditional Stam]
If the Cubic Control Inequality holds, then the full Finite Free Stam Inequality holds.
\end{theorem}

%==============================================================================
\section{Alternative Characterizations}
%==============================================================================

The Stam inequality can be reformulated in several instructive ways that highlight different aspects of the convolution process.

\subsection{The Efficiency Ratio and Regularization}

A natural way to measure the "non-Gaussianity" of a polynomial's root distribution is by comparing its Fisher information to the lower bound established in Lemma~\ref{lem:fv}.

\begin{definition}[Efficiency Ratio]
For $p \in \PnR$ with positive variance, the \emph{efficiency ratio} $\eta(p)$ is defined as:
\[
\eta(p) \coloneqq \frac{4\Phi_n(p) \sigma^2(p)}{n(n-1)^2}.
\]
By the Fisher--Variance inequality, $\eta(p) \ge 1$, with equality if and only if $n=2$ (or in the $n \to \infty$ Gaussian limit for random polynomials).
\end{definition}

In this language, the Stam inequality is related to the following regularization property.

\begin{theorem}[Regularization Property]
For $p, q \in \PnR$ with positive variances, the efficiency ratio of the convolution satisfies:
\[
\eta(p \boxplus_n q) \le \frac{\sigma^2(p)\eta(p) + \sigma^2(q)\eta(q)}{\sigma^2(p) + \sigma^2(q)}.
\]
\end{theorem}

This theorem asserts that the efficiency ratio of the convolution is bounded by the variance-weighted average of the efficiency ratios of the components. Combined with the convexity of the map $x \mapsto 1/x$, this regularization directly implies the full Stam inequality.

\subsection{Convex Mixing and Matrix-Level Jensen's Inequality}

The symmetric additive convolution can be viewed as a form of convex mixing at the level of the matrix model. Let $A$ and $B$ be centered symmetric matrices representing $p$ and $q$. The functional $\Psi_n(M) = \sigma^2(M)\Phi_n(\chi_M)$ is scale-invariant and satisfies a matrix-level Jensen's inequality.

\begin{proposition}[Convexity under Haar Averaging]
For any $t \in [0,1]$, the expected efficiency of the interpolation satisfies:
\[
\E_{Q \sim \mathrm{Haar}(O(n))} [\Psi_n(tA + (1-t)QBQ^T)] \le t \Psi_n(A) + (1-t)\Psi_n(B).
\]
\end{proposition}

This characterization reveals that the "smoothing" effect of convolution is fundamentally a consequence of the convexity of the Fisher information functional. The integration over the orthogonal group $O(n)$ acts as a mixing process that averages the "stresses" of the root configurations, leading to a state of lower aggregate Fisher information.

%==============================================================================
\section{Conclusion}
%==============================================================================

We have established a framework linking the geometry of polynomial roots to information-theoretic inequalities.
\begin{enumerate}
    \item We defined root interactions via electrostatic forces.
    \item We demonstrated that convolution minimizes interaction energy.
    \item We derived a precise integral formula for the increase in smoothness.
    \item We proved the Half-Stam inequality unconditionally.
\end{enumerate}

The validity of the full Stam inequality rests on the Cubic Control Conjecture, which captures the subtle higher-order symmetries of the root configuration.

%==============================================================================
% Bibliography
%==============================================================================
\begin{thebibliography}{9}
\bibitem{MSS15} A.~Marcus, D.~Spielman, N.~Srivastava,
\emph{Interlacing families II: Mixed characteristic polynomials and the Kadison-Singer problem},
Ann.\ Math.\ 182 (2015).

\bibitem{Stam59} A.~J.~Stam,
\emph{Some inequalities satisfied by the quantities of information of Fisher and Shannon},
Information and Control (1959).
\end{thebibliography}

\end{document}
